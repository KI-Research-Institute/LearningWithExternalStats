---
title: "Evaluating Performance Using External Statistics with CDM"
output:
  pdf_document:
    number_sections: yes
    toc: yes
  html_document:
    number_sections: yes
    toc: yes
vignette: >
  %\VignetteIndexEntry{Using external evaluation with CDM}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Overview
This example demonstrate that usage of `LearningWithExternalStats` package with
[patient level prediction (PLP)](https://ohdsi.github.io/PatientLevelPrediction/).
To allow working with PLP, we will use another supporting package, `plpDataAdapter`.

The stages of this demonstration are roughly organized according to the specific roles
of the internal and external nodes.

# Setup

The following definitions set the ground to working with `Eunomia` simulated data set:

```{r setup, message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
library(Eunomia)  # v1.0.2
library(FeatureExtraction)  # v3.2.0
library(PatientLevelPrediction)  # v6.0.4
library(glue)
library(plpDataAdapter)
suppressPackageStartupMessages(library(LearningWithExternalStats))
# TODO: currently loading this library with the former ones creates the following:
# Found more than one class "atomicVector" in cache; using the first, from namespace 'Matrix'
# Also defined by 'Rmpfr'

# Activate Eunomia demo database
connectionDetails <- Eunomia::getEunomiaConnectionDetails()
Eunomia::createCohorts(connectionDetails)
# Data definitions - cohort IDs
outcomeId <- 3
internalTargetId <- 1  # Celecoxib users
externalTargetId <- 2  # Diclofenac users
```

Data settings will be shared among internal and external node except for `targetId` which is node specific.

```{r dataSettings, message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
covSettings <- createCovariateSettings(
  useDemographicsGender = TRUE,
  useDemographicsAge = TRUE,
  useConditionGroupEraLongTerm = TRUE,
  useConditionGroupEraAnyTimePrior = TRUE,
  useDrugGroupEraLongTerm = TRUE,
  useDrugGroupEraAnyTimePrior = TRUE,
  useVisitConceptCountLongTerm = TRUE,
  longTermStartDays = -365,
  endDays = -1)
databaseDetails <- createDatabaseDetails(
  connectionDetails = connectionDetails,
  cdmDatabaseSchema = "main",
  cdmDatabaseName = '',
  cdmDatabaseId = '', 
  tempEmulationSchema = NULL,
  cohortDatabaseSchema = "main",
  cohortTable = "cohort",
  targetId = internalTargetId,
  outcomeDatabaseSchema = "main",
  outcomeTable = "cohort",
  outcomeIds = outcomeId,
  cdmVersion = 5
)
restrictPlpDataSettings <- createRestrictPlpDataSettings()
```

# Internal node: model training and sharing active model covariates
We begin by defining prediction model settings:
```{r modelSettings, message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
populationSettings <- createStudyPopulationSettings(
  washoutPeriod = 364,
  firstExposureOnly = FALSE,
  removeSubjectsWithPriorOutcome = TRUE,
  priorOutcomeLookback = 9999,
  riskWindowStart = 1,
  riskWindowEnd = 365,
  minTimeAtRisk = 364,
  requireTimeAtRisk = TRUE,
  includeAllOutcomes = TRUE)
splitSettings <- createDefaultSplitSetting(
  trainFraction = 0.75,
  testFraction = 0.25,
  type = 'stratified',
  nfold = 2,
  splitSeed = 1234
)
sampleSettings <- createSampleSettings()
featureEngineeringSettings <- createFeatureEngineeringSettings()
preprocessSettings <- createPreprocessSettings(
  minFraction = 0.01,
  normalize = T,
  removeRedundancy = T
)
lrModel <- setLassoLogisticRegression()
executeSettings = createExecuteSettings(
  runSplitData = T,
  runSampleData = T,
  runfeatureEngineering = T,
  runPreprocessData = T,
  runModelDevelopment = T,
  runCovariateSummary = T
)
```
Next we generate the internal data:
```{r generateInternal, message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
internalPlpData <- getPlpData(
    databaseDetails = databaseDetails,
    covariateSettings = covSettings,
    restrictPlpDataSettings = restrictPlpDataSettings)
```

We train the model and extract its active covariates:

```{r message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
internalResults <- runPlp(
    plpData = internalPlpData,
    outcomeId = outcomeId,
    analysisId = '../singleDemo',
    analysisName = 'Demonstration of model training in internal node',
    populationSettings = populationSettings,
    splitSettings = splitSettings,
    sampleSettings = sampleSettings,
    featureEngineeringSettings = featureEngineeringSettings,
    preprocessSettings = preprocessSettings,
    modelSettings = lrModel,
    logSettings = createLogSettings(),
    executeSettings = executeSettings,
    saveDirectory = file.path(getwd(), '..', 'singlePlp')
  )
# Identify model covariates. These will be used in the estimation algorithm.
estimationCovariates <- getLRModelCovariates(internalResults)
```

Preliminary experiments have shown that using the subset selected by the model for generation of external statistics
gives better results than using all covariates.

```{r show results}
cat(glue('Selected {length(estimationCovariates)} features'), '\n')
```
The external node should receive now the estimation covariates.

\newpage
# External node
The external nodes generate PLP data and prepares statistics for sharing with the internal one. For the sake of
algorithm evaluation it also evaluate the model trained on the internal node.

## Generate PLP data and evaluate model
```{r message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
externalDatabaseDetails <- createDatabaseDetails(
  connectionDetails = connectionDetails,
  cdmDatabaseSchema = "main",
  cdmDatabaseName = '',
  cdmDatabaseId = '',
  tempEmulationSchema = NULL,
  cohortDatabaseSchema = "main",
  cohortTable = "cohort",
  targetId = externalTargetId,
  outcomeDatabaseSchema = "main",
  outcomeTable = "cohort",
  outcomeIds = outcomeId,
  cdmVersion = 5
)
externalPlpData <- getPlpData(
  databaseDetails = externalDatabaseDetails,
  covariateSettings = covSettings,
  restrictPlpDataSettings = restrictPlpDataSettings)
externalResults <- externalValidateDbPlp(  # TODO avoid duplicated generation of plpData
  internalResults$model,
  validationDatabaseDetails = externalDatabaseDetails)
```

## Prepare statistics for sharing

```{r message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
externalXY <- plpDataAdapter::transformPlpDataToDataFrame(
  externalPlpData, populationSettings, outcomeId = outcomeId)
ZExt <- computeTable1LikeTransformation(
  externalXY[c(estimationCovariates, 'outcome')],
  outcomeBalance = T,
  outcomeCol = 'outcome'
)
muExt <- colMeans(ZExt)
```
These statistics are sent to the internal node.

\newpage

# Internal node: evaluation of external dataset peformence using internal dataset

First the data is adapted and transformed:

```{r message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
# Transform internal plp data and align it with predictions
internalXY <- plpDataAdapter::transformPlpDataToDataFrame(
  internalPlpData, populationSettings, outcomeId = outcomeId)
allignedInternal <- plpDataAdapter::allignDataAndPrediction(
  internalXY, internalResults, subsets = 'Test')
# Compute transformation on covariate outcome pairs
ZInt <- computeTable1LikeTransformation(
  allignedInternal$dataXY[c(estimationCovariates, 'outcome')],
  outcomeBalance = T,
  outcomeCol = 'outcome'
)
# Estimate external performance
internalData <- list(
  z=ZInt,
  p = allignedInternal$prediction$value,
  y = allignedInternal$dataXY[['outcome']]
)
```

Before we estimate, we should define parameters of the external estimation algorithms. 
The first set of parameters determines the properties of the re-weighting algorithm. Specifically,
`divergence` parameter defines the type of divergence between the
weighted distribution and the uniform distribution. `lambda` controls
the tradeoff between matching accuracy and closeness to the uniform
distribution. `minW` is the minimum allowed weight. `optimizationMethod`
can be `'dual'` or `'primal'`.

The other set determines additional factors. 

As the re-weighting algorithm
can handle sets with size of up to a few 10,000 samples, good estimation can be obtained by sub-sampling.
`nMaxReweight` determines that size of the subsamples. `stratifiedSampling` deals with rare outcomes and if set to
`TRUE` the algorithm sub-samples the two classes independently. `nRepetitions` is the number of repetitions of running
the reweighting algorithm. In case `nMaxReweight` is smaller than the sample size, subsampling is performed without
replacement and the final estimation and confidence measures are obtained from the repetitions summary, 
otherwise, the main estimates are obtained from the entire set and the confidence measures are obtained by 
bootstrapping. `maxProp` is the maximum allowed proportion between internal and external frequencies of binary
variables in cases that the difference between proportions is larger than 0.01. `outputDir` is the directory
used for logging. `maxCores` are the maximum cores used in parallel for repeated sub-sampling. 


```{r message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
externalEstimatorSettings <- createExternalEstimatorSettings(
  divergence = 'entropy',  # entropy, chi2
  lambda = 1e-1,
  minW = 0,
  optimizationMethod = 'dual',  # dual, primal
  nMaxReweight = 10000,
  stratifiedSampling = T,
  nRepetitions = 10,
  maxProp = 100,
  outputDir = getwd(),
  maxCores = 3
  )
estimatedResults <- estimateExternalPerformanceFromStatistics(
  internalData = internalData,
  externalStats = muExt,
  externalEstimatorSettings = externalEstimatorSettings
)
```

# Compare evaluation and real results

```{r message=FALSE, warning=FALSE, eval=TRUE}
plpMetrics <- c(
  'populationSize','outcomeCount','AUROC','95% lower AUROC','95% upper AUROC',
  'brier score','calibrationInLarge mean prediction','calibrationInLarge observed risk'
)
intPerf <- internalResults$performanceEvaluation$evaluationStatistics
rows <- (intPerf['evaluation']=='Test') & (intPerf[['metric']] %in% plpMetrics)
cat('Internal performance:\n')
print(format(intPerf[rows, c('metric', 'value')], digits=3))

extPerf <- externalResults[[2]]$performanceEvaluation$evaluationStatistics
rows <- (extPerf['evaluation']=='Validation') & (extPerf[['metric']] %in% plpMetrics)
cat('External performance:\n')
print(format(extPerf[rows, c('metric', 'value')], digits=3))

cat('Estimated exteranl performance','\n')
showResults <- c(
  'n', 
  'n outcome',
  'Max weight',
  'chi2 to uniform',
  'kl',
  'Max Weighted SMD',
  'AUROC', 
  'Rough 95% lower AUROC', 
  'Rough 95% upper AUROC', 
  'n repetitions',
  'Brier score',
  'Global calibration mean prediction',
  'Global calibration observed risk')
estimationView <- estimatedResults$estimation[showResults, , drop = F]
# Format the output:
estimationView[, 'value'] <- apply(estimationView, 1, function(x) {sprintf('%.3g', x)})
estimationView['n', 'value'] <- estimatedResults$estimation['n', 'value'] 
print(estimationView)
```
Here, `n` is the number of samples that recieved a weight greater than zero;
`Max weight` is the maximal sample weight of the weight vector generated by the rewighing algorithm;
`chi2 to uniform` and `kl` are the chi squared and Kullback-Leibler divergences between the obtained
weights and the uniform weights respectively; and `Max Weighted SMD` is the maximal weighted
standard mean difference between covariates in the external sample and the reweighted sample.
