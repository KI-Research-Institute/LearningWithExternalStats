---
title: "Evaluating Performance Using External Statistics with CDM"
output:
  pdf_document:
    number_sections: yes
    toc: yes
  html_document:
    number_sections: yes
    toc: yes
vignette: >
  %\VignetteIndexEntry{Using external evaluation with CDM}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Overview
This example demonstrate that usage of `LearningWithExternalStats` package with
[patient level prediction (PLP)](https://ohdsi.github.io/PatientLevelPrediction/).
To allow working with PLP, we will use another supporting package, `plpDataAdapter`.

The stages of this demonstration are roughly organized according to the specific roles
of the internal and external nodes.

# Setup

The following definitions set the ground to working with `Eunomia` simulated data set:

```{r setup, message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
library(glue)
library(plpDataAdapter)
library(LearningWithExternalStats)
library(Eunomia)  # v1.0.2
library(FeatureExtraction)  # v3.2.0
library(PatientLevelPrediction)  # v6.0.4

# Activate Eunomia demo database
connectionDetails <- Eunomia::getEunomiaConnectionDetails()
Eunomia::createCohorts(connectionDetails)
# Data definitions - cohort IDs
outcomeId <- 3
internalTargetId <- 1  # Celecoxib users
externalTargetId <- 2  # Diclofenac users
```

Data settings will be shared among internal and external node except for `targetId` which is node specific.

```{r dataSettings, message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
covSettings <- createCovariateSettings(
  useDemographicsGender = TRUE,
  useDemographicsAge = TRUE,
  useConditionGroupEraLongTerm = TRUE,
  useConditionGroupEraAnyTimePrior = TRUE,
  useDrugGroupEraLongTerm = TRUE,
  useDrugGroupEraAnyTimePrior = TRUE,
  useVisitConceptCountLongTerm = TRUE,
  longTermStartDays = -365,
  endDays = -1)
databaseDetails <- createDatabaseDetails(
  connectionDetails = connectionDetails,
  cdmDatabaseSchema = "main",
  cdmDatabaseName = '',
  cdmDatabaseId = '', 
  tempEmulationSchema = NULL,
  cohortDatabaseSchema = "main",
  cohortTable = "cohort",
  targetId = internalTargetId,
  outcomeDatabaseSchema = "main",
  outcomeTable = "cohort",
  outcomeIds = outcomeId,
  cdmVersion = 5
)
restrictPlpDataSettings <- createRestrictPlpDataSettings()
```

# Internal node: model training and sharing active model covariates
We begin by defining prediction model settings:
```{r modelSettings, message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
populationSettings <- createStudyPopulationSettings(
  washoutPeriod = 364,
  firstExposureOnly = FALSE,
  removeSubjectsWithPriorOutcome = TRUE,
  priorOutcomeLookback = 9999,
  riskWindowStart = 1,
  riskWindowEnd = 365,
  minTimeAtRisk = 364,
  requireTimeAtRisk = TRUE,
  includeAllOutcomes = TRUE)
splitSettings <- createDefaultSplitSetting(
  trainFraction = 0.75,
  testFraction = 0.25,
  type = 'stratified',
  nfold = 2,
  splitSeed = 1234
)
sampleSettings <- createSampleSettings()
featureEngineeringSettings <- createFeatureEngineeringSettings()
preprocessSettings <- createPreprocessSettings(
  minFraction = 0.01,
  normalize = T,
  removeRedundancy = T
)
lrModel <- setLassoLogisticRegression()
executeSettings = createExecuteSettings(
  runSplitData = T,
  runSampleData = T,
  runfeatureEngineering = T,
  runPreprocessData = T,
  runModelDevelopment = T,
  runCovariateSummary = T
)
```
Next we generate the internal data:
```{r generateInternal, message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
# TODO: currently CVXR used by LearningWithExternalStats causes:
# Found more than one class "atomicVector" in cache; using the first, from namespace 'Matrix'
# Also defined by 'Rmpfr'
internalPlpData <- getPlpData(
    databaseDetails = databaseDetails,
    covariateSettings = covSettings,
    restrictPlpDataSettings = restrictPlpDataSettings)
```

We train the model and extract its active covariates:

```{r message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
internalResults <- runPlp(
    plpData = internalPlpData,
    outcomeId = outcomeId,
    analysisId = '../singleDemo',
    analysisName = 'Demonstration of model training in internal node',
    populationSettings = populationSettings,
    splitSettings = splitSettings,
    sampleSettings = sampleSettings,
    featureEngineeringSettings = featureEngineeringSettings,
    preprocessSettings = preprocessSettings,
    modelSettings = lrModel,
    logSettings = createLogSettings(),
    executeSettings = executeSettings,
    saveDirectory = file.path(getwd(), '..', 'singlePlp')
  )
# Identify model covariates. These will be used in the estimation algorithm.
estimationCovariates <- getLRModelCovariates(internalResults)
```

Preliminary experiments have shown that using the subset selected by the model for generation of external statistics
gives better results than using all covariates.

```{r show results}
cat(glue('Selected {length(estimationCovariates)} features'), '\n')
```
The external node should receive now the estimation covariates.

\newpage
# External node
The external nodes generate PLP data and prepares statistics for sharing with the internal one. For the sake of
algorithm evaluation it also evaluate the model trained on the internal node.

## Generate PLP data and evaluate model
```{r message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
externalDatabaseDetails <- createDatabaseDetails(
  connectionDetails = connectionDetails,
  cdmDatabaseSchema = "main",
  cdmDatabaseName = '',
  cdmDatabaseId = '',
  tempEmulationSchema = NULL,
  cohortDatabaseSchema = "main",
  cohortTable = "cohort",
  targetId = externalTargetId,
  outcomeDatabaseSchema = "main",
  outcomeTable = "cohort",
  outcomeIds = outcomeId,
  cdmVersion = 5
)
externalPlpData <- getPlpData(
  databaseDetails = externalDatabaseDetails,
  covariateSettings = covSettings,
  restrictPlpDataSettings = restrictPlpDataSettings)
externalResults <- externalValidateDbPlp(  # TODO avoid duplicated generation of plpData
  internalResults$model,
  validationDatabaseDetails = externalDatabaseDetails)
```

## Prepare statistics for sharing

```{r message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
externalXY <- plpDataAdapter::transformPlpDataToDataFrame(
  externalPlpData, populationSettings, outcomeId = outcomeId)
ZExt <- computeTable1LikeTransformation(
  externalXY[c(estimationCovariates, 'outcome')],
  outcomeBalance = T,
  outcomeCol = 'outcome'
)
muExt <- colMeans(ZExt)
```
These statistics are sent to the internal node.

\newpage

# Internal node: evaluation of external dataset peformence using internal dataset

First the data is adapted and transformed:

```{r message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
# Transform internal plp data and align it with predictions
internalXY <- plpDataAdapter::transformPlpDataToDataFrame(
  internalPlpData, populationSettings, outcomeId = outcomeId)
allignedInternal <- plpDataAdapter::allignDataAndPrediction(
  internalXY, internalResults, subsets = 'Test')
# Compute transformation on covariate outcome pairs
ZInt <- computeTable1LikeTransformation(
  allignedInternal$dataXY[c(estimationCovariates, 'outcome')],
  outcomeBalance = T,
  outcomeCol = 'outcome'
)
# Estimate external performance
internalData <- list(
  z = ZInt,
  p = allignedInternal$prediction$value,
  y = allignedInternal$dataXY[['outcome']]
)
```

Before we estimate, we should define parameters of the external estimation algorithms. 
`nRepetitions` is the number of repetitions of running
the reweighting algorithm for bootstrapped confidece interval extraction. `outputDir` is the directory
used for logging and `maxCores` are the maximum cores used in parallel for repeated sub-sampling. 


```{r message=FALSE, warning=FALSE, eval=TRUE, results='hide'}
externalEstimatorSettings  <- createExternalEstimatorSettings(
  reweightAlgorithm = seTunedWeightOptimizer(),
  nRepetitions = 100,
  outputDir = getwd(),
  maxCores = 15
)


estimatedResults <- estimateExternalPerformanceFromStatistics(
  internalData = internalData,
  externalStats = muExt,
  externalEstimatorSettings = externalEstimatorSettings
)
```

# Compare evaluation and real results

```{r message=FALSE, warning=FALSE, eval=TRUE}
plpMetrics <- c(
  'populationSize','outcomeCount','AUROC','95% lower AUROC','95% upper AUROC',
  'brier score','calibrationInLarge mean prediction','calibrationInLarge observed risk'
)
intPerf <- internalResults$performanceEvaluation$evaluationStatistics
rows <- (intPerf['evaluation']=='Test') & (intPerf[['metric']] %in% plpMetrics)
cat('Internal performance:\n')
print(format(intPerf[rows, c('metric', 'value')], digits=3))

extPerf <- externalResults[[2]]$performanceEvaluation$evaluationStatistics
rows <- (extPerf['evaluation']=='Validation') & (extPerf[['metric']] %in% plpMetrics)
cat('External performance:\n')
print(format(extPerf[rows, c('metric', 'value')], digits=3))

cat('External estimation status =', estimatedResults$status, '\n')
```

When the status is `'Success'` the results are given in sub-field `estimation` (otherwise it may be `NULL`):

```{r message=FALSE, warning=FALSE, eval=TRUE}
if (estimatedResults$status=='Success') {
  cat('Estimated exteranl performance','\n')
  estimationView <- estimatedResults$estimation
  # Format the output:
  estimationView[, 'value'] <- apply(estimationView, 1, function(x) {sprintf('%.3g', x)})
  print(estimationView)
}
```

# Assessing diagnostics

More comprehensive results are in the `results` field:
```{r message=FALSE, warning=FALSE, eval=TRUE}
cat('All results include diagnostics and estimated exteranl performance','\n')
resultsView <- estimatedResults$results
resultsView[, 'value'] <- apply(resultsView, 1, function(x) 
  {if (suppressWarnings(!is.na(as.numeric(x)))) sprintf('%.3g',as.numeric(x)) else x})  
print(resultsView)
```

Here, `n` is the number of samples that recieved a weight greater than zero;
`Max weight` is the maximal sample weight of the weight vector generated by the rewighing algorithm;
`chi2 to uniform` and `kl` are the chi squared and Kullback-Leibler divergences between the obtained
weights and the uniform weights respectively; and `Max Weighted SMD` is the maximal weighted
standard mean difference between covariates in the external sample and the reweighted sample.

# A positivity violation example

Let's create an artificial imbalance
```{r message=FALSE, warning=FALSE, eval=TRUE}
c1 <- estimationCovariates[1]
allignedInternal$dataXY[[c1]] <- 0
ZInt <- computeTable1LikeTransformation(
  allignedInternal$dataXY[c(estimationCovariates, 'outcome')],
  outcomeBalance = T,
  outcomeCol = 'outcome'
)
internalData <- list(
  z = ZInt,
  p = allignedInternal$prediction$value,
  y = allignedInternal$dataXY[['outcome']]
)
```
Next we rerun the estimation
```{r message=FALSE, warning=FALSE, eval=TRUE}
estimatedResults2 <- estimateExternalPerformanceFromStatistics(
  internalData = internalData,
  externalStats = muExt,
  externalEstimatorSettings = externalEstimatorSettings
)
print(estimatedResults2$results[
  rownames(estimatedResults2$results) != 'incompatibleUnaryRepresentative', , drop = F])
```
and observe a reason for lack of overlap
```{r message=FALSE, warning=FALSE, eval=TRUE}
cat(estimatedResults2$results['incompatibleUnaryRepresentative', ], '\n')
```

# Aggregating results from multiple runs

The following utility functions allow easy initialization of a data frame that aggregates results
```{r message=FALSE, warning=FALSE, eval=TRUE}
fields <- c(getPreDiagnosticsFieldNames(), getWeightingResultsFieldNames(), getEstimationFieldNames(),
            'Internal AUC', 'External AUC')
summary <- data.frame(row.names = fields)
```
Now we can assign specific experiments results and present the estimation:
```{r message=FALSE, warning=FALSE, eval=TRUE}
summary['Internal AUC', 'Exp 1'] <- 
  intPerf[intPerf[['metric']] == 'AUROC' & intPerf[['evaluation']] == 'Test', 'value']
summary['External AUC', 'Exp 1'] <- 
  extPerf[extPerf[['metric']] == 'AUROC' & extPerf[['evaluation']] == 'Validation', 'value']
summary[rownames(estimatedResults$results), 'Exp 1'] <- estimatedResults$results
summary[rownames(estimatedResults2$results), 'Exp 2'] <- estimatedResults2$results
print(summary[getEstimationFieldNames(), ])
```
To see why Exp 2 does not have an esitimation, we can view the pre-diagnosis results:
```{r message=FALSE, warning=FALSE, eval=TRUE}
print(summary[getPreDiagnosticsFieldNames(), ])
```
And we can view the weighting summary
```{r message=FALSE, warning=FALSE, eval=TRUE}
print(summary[getWeightingResultsFieldNames(), ])
```


